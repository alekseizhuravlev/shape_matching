{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "con = sqlite3.connect(\"/home/s94zalek_hpc/shape_matching/my_code/experiments/ddpm/log_p2p_median_dirichlet.db\")\n",
    "cur = con.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"CREATE TABLE ddpm(experiment_name, checkpoint_name, smoothing, dataset_name, split, dirichlet, p2p_median, zoomout_mean, zoomout_median, pred_mean, pred_median)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "err_list = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"test\", 'chkpt_99', 'no', 'FAUST_orig_pair', 'test',\n",
    "     10, 15,\n",
    "     err_list.mean().item(), err_list.median().item(),\n",
    "     err_list.mean().item(), err_list.median().item()),\n",
    "]\n",
    "cur.executemany(\"INSERT INTO ddpm VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\", data)\n",
    "con.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT * FROM ddpm\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "with sqlite3.connect(\"/home/s94zalek_hpc/shape_matching/my_code/experiments/ddpm/log_p2p_median_dirichlet.db\") as con:\n",
    "    df = pd.read_sql_query(\"SELECT * FROM ddpm\", con)\n",
    "    \n",
    "df.sort_values(['dataset_name', 'p2p_median', 'experiment_name'], ascending=True, inplace=True)\n",
    "\n",
    "# for each unique dataset_name, show 5 best experiments\n",
    "# df.groupby('dataset_name').apply(lambda x: x.nsmallest(8, 'p2p_median')).reset_index(drop=True)\n",
    "\n",
    "# for each unique dataset_name, show 3 best experiments with checkpoint_name = 'epoch_99' and 3 best experiments with checkpoint_name = 'checkpoint_99.pt'\n",
    "# df_epoch_99 = df[df['checkpoint_name'] == 'epoch_99'].copy()\n",
    "# df_checkpoint_99 = df[df['checkpoint_name'] == 'checkpoint_99.pt'].copy()\n",
    "\n",
    "# df_epoch_99.groupby('dataset_name').apply(lambda x: x.nsmallest(3, 'p2p_median')).reset_index(drop=True)\n",
    "# df_checkpoint_99.groupby('dataset_name').apply(lambda x: x.nsmallest(3, 'p2p_median')).reset_index(drop=True)\n",
    "\n",
    "# # combine both dataframes\n",
    "# df_combined = pd.concat([df_epoch_99, df_checkpoint_99])\n",
    "\n",
    "# df_combined.sort_values(['dataset_name', 'p2p_median', 'experiment_name'], ascending=True, inplace=True)\n",
    "\n",
    "# df_combined\n",
    "\n",
    "# round all numbers  to 2 decimal places\n",
    "df = df.round(2)\n",
    "\n",
    "# only keep rows with '24' in the experiment_name\n",
    "# df = df[df['experiment_name'].str.contains('48') | (df['experiment_name'] == 'single_template_remeshed')]\n",
    "\n",
    "# highlight the rows with 'single_template_remeshed_augShapes' in the experiment_name\n",
    "df.style.apply(lambda x: ['background: lightgreen' if x['experiment_name'].str.contains('single_template_remeshed_augShapes') else '' for i in x], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "df.to_csv('/home/s94zalek_hpc/shape_matching/my_code/experiments/ddpm/log_p2p_median_dirichlet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur.executemany(\n",
    "#     \"SELECT * FROM ddpm WHERE experiment_name=? AND checkpoint_name=? AND smoothing=? AND dataset_name=? AND split=?\",\n",
    "#     [('pair_10_xy_distributed',\n",
    "#       'epoch_99',\n",
    "#       'no',\n",
    "#       'FAUST_a_pair',\n",
    "#       'test'\n",
    "#       )]).fetchone()\n",
    "\n",
    "if cur.execute(f\"SELECT * FROM ddpm WHERE experiment_name='pair_10_xy_distributed' AND checkpoint_name='epoch_99' AND dataset_name='FAUST_orig_pair' AND split='test' AND smoothing='no'\").fetchall():\n",
    "    cur.execute(f\"DELETE FROM ddpm WHERE experiment_name='pair_10_xy_distributed' AND checkpoint_name='epoch_99' AND dataset_name='FAUST_orig_pair' AND split='test' AND smoothing='no'\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "l[:int(round(1.6, 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all files from /home/s94zalek_hpc/shape_matching/data/SHREC19_r/corres\n",
    "import os\n",
    "\n",
    "files = os.listdir(\"/home/s94zalek_hpc/shape_matching/data/SHREC19_r/corres\")\n",
    "\n",
    "files[290]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read each yaml file in /home/s94zalek_hpc/shape_matching/my_code/experiments/ddpm_results\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "\n",
    "base_dir = \"/home/s94zalek_hpc/shape_matching/my_code/experiments/ddpm_results\"\n",
    "# logs_folder_name = \"logs_robustMedian_fixedSmoothing_64_10_0.2\"\n",
    "logs_folder_name = \"logs_robustMedian_fixedSmoothing_128_16_0.2\"\n",
    "\n",
    "# logs_folder_name = \"test_partial_on_train_data\"\n",
    "# logs_folder_name = \"test_partial_on_train_data_1_0.5_0.9\"\n",
    "# logs_folder_name='logs_templateZoomout'\n",
    "# logs_folder_name='logs_partial_symm'\n",
    "# logs_folder_name='logs_partial_symm_zo'\n",
    "\n",
    "yaml_files = os.listdir(f\"{base_dir}/{logs_folder_name}\")\n",
    "yaml_data = []\n",
    "\n",
    "for file in yaml_files:\n",
    "    with open(f\"{base_dir}/{logs_folder_name}/{file}\", 'r') as f:\n",
    "        yaml_data.append(yaml.load(f, Loader=yaml.FullLoader))\n",
    "\n",
    "# convert yaml_data to a pandas dataframe\n",
    "df = pd.DataFrame(yaml_data)\n",
    "df.sort_values(['dataset_name', 'median_pairzo', 'experiment_name'], ascending=True, inplace=True)\n",
    "\n",
    "# remove entries with dataset_name = 'DT4D_inter_pair' and 'DT4D_intra_pair'\n",
    "# df = df[(df['dataset_name'] != 'DT4D_inter_pair') & (df['dataset_name'] != 'DT4D_intra_pair')]\n",
    "\n",
    "# if 'smoothing_type' in df.columns:\n",
    "#     df = df[(df['smoothing_type'] != 'taubin')]\n",
    "# if 'smoothing' in df.columns:\n",
    "#     df = df[(df['smoothing'] != 'taubin-5')]\n",
    "\n",
    "# df.sort_values(['dataset_name', 'geo_err_est_rev_mean', 'experiment_name'], ascending=True, inplace=True)\n",
    "\n",
    "\n",
    "# df = df[df['experiment_name'].str.contains('single_128')]\n",
    "\n",
    "# remove columns 'auc', 'pcks', 'thresholds'\n",
    "\n",
    "df = df.drop(columns=['auc', 'pcks', 'thresholds'])\n",
    "\n",
    "df = df.round(1)\n",
    "\n",
    "# df = df[(df['dataset_name'] == 'SHREC19_r_pair')]\n",
    "\n",
    "# do not show nans\n",
    "# df[['experiment_name', 'dataset_name', 'median_nozo']].dropna()\n",
    "# df[['experiment_name', 'dataset_name', 'median_pairzo']].dropna()\n",
    "# df[['experiment_name', 'dataset_name', 'dirichlet_pairzo']].dropna()\n",
    "df[['experiment_name', 'dataset_name', 'zoomout_mean']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current time in readable format, for use as a filename\n",
    "import datetime\n",
    "\n",
    "(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"), )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "chkpt = torch.load('/lustre/mlnvme/data/s94zalek_hpc-shape_matching/ddpm_checkpoints/single_template_remeshed/checkpoints/checkpoint_99.pt', map_location='cpu')\n",
    "\n",
    "chkpt.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
